---
layout:     post
title:      Logistic 回归简介
subtitle:   Logistic Regression
date:       2013-04-02
author:     BitLines
header-img: img/post-bg-blog.jpeg
catalog: true
tags:
    - 经典机器学习
    - 机器学习
---

## Logistic 回归简介
大家常说的 LR 就是 Logistic 回归，中文名叫逻辑回归或者逻辑斯特回归。别被 Logistic 回归的名字骗了，他才不是回归方法，而是分类方法。Logistic 回归主要是用来解决二分类问题。比如：
- 通过邮件内容判断一封邮件是否为垃圾邮件
- 给定用户描述的症状判别是否患有流感
- 给定一只股票最近7点的涨跌情况判断明天是否会涨

Logistic 回归是二分类问题，判别样本是正样本还是负样本。背后的想法是从样本中提取特征转化成特征向量，这样可以在多维空间中把所有标注样本描绘出来。Logistic 回归的学习目标是从多维空间中寻找一个分类超平面，使得超平面尽可能把正负样本分离开，以二维空间举例，如下图：

![image](https://user-images.githubusercontent.com/80689631/111463676-ccefb500-875a-11eb-8df0-c934abec6d81.png)

## 数学介绍
Logistic 回归模型的输入是特征向量 $x \in R^n$,  输出是 $y \in \{0, 1\}$，模型参数是 $w \in R^n$ 和 $b \in R$, 模型建模的是 $P(y|x)$，如下：

$$ P(y=1|x)=\frac{e^{(w^Tx + b)}}{e^{(w^Tx + b)}+1} $$
$$ P(y=0|x)=\frac{1}{e^{(w^Tx + b)}+1} $$

Logistic 回归的学习目标是极大似然估计，即使所见样本（标注样本或者叫做观测结果）的对数似然概率最大化：

$$ \max_{w, b}{\sum_{i=1}^{n}{logP(Y=y_i|x_i)}} $$


这样解释不容易懂。说一下具体的例子：
> 例如有 10 个样本 $\{(x_1, y_1),..., (x_{10}, y_{10})\}$，最开始随机初始化参数 $w$和$b$，这样可以计算出每个样本的实际概率 $\{P(Y=y_1|x_1),... ,P(Y=y_{10}|x_{10})\}$。数学问题就转化成了 求 $w$ 和 $b$ 使得 $\sum_{i=1}^{10}{log{P(Y=y_i|x_i)}}$最大。

模型求解一般采用梯度下降（批量梯度下降或随机梯度下降）。

## 实际应用

**案例一：垃圾邮件分类**  
垃圾邮件分类，模型的输入通常是三元组 $(E,T,C)$，其中 $E$ 表示发件人， $T$ 表示邮件标题， $C$ 邮件内容。在实际场景中需要抽样出一批数据进行标注，垃圾邮件作为正样本，普通邮件作为负样本。垃圾邮件分类先通过特征工程把 $(E,T,C)$ 转化为特征向量 $x$，然后建立Logistic 回归模型求解参数 $w$ 和 $b$。

**案例二：广告点击预估**  
广告点击率预估，模型的输入通常是三元组 $(U,Q,A)$，其中 $U$ 表示用户画像， $Q$ 表示用户的查询 Query， $A$ 表示广告特征。在线上系统中可以收集到大量的用户查询后点击和未点击的数据，点击数据作为正样本，未点击数据作为负样本。广告点击预测模型通常是先通过特征工程把 $(U,Q,A)$ 转化为特征向量 $x$，然后建立Logistic 回归模型求解参数 $w$ 和 $b$。

## Logistic 回归的优缺点

**优点：**
- 训练收敛快，存在全局最优解
- 计算量小，运行效率高，常用于工业界对时延要求严苛的系统中
- 特征可解释，特征权重越大，对分类判定影响越大。

**缺点：**
- 解决问题依赖特征工程，限制了成功经验的复制。
- 容易欠拟合，一般准确度不太高
- 只适用于数据特征线性可分的场景