## 强化学习经典方法

### 基于值的方法
- Value Iteration
- DQN
- DDQN

### 基于策略的方法

- Policy Iteration
- PG(Policy Gradient)
- TRPO
- PPO



## 强化学习概念

### 1. 区分 有监督学习/无监督学习/模仿学习/强化学习
- 有监督学习：$$
- 无监督学习：
- 模仿学习：
- 强化学习：

### 2. 区分 Return(s,a) 和 reward(s,a)
- $reward(s,a)$ 是 environment 在状态 $s$ 下，对行为 $a$ 的单步立即奖励值。
- $Return(s,a)$ 是 在状态 $s$ 下，长期期望奖励值。

### 3. 区分 $Return$, $Q$ , $V$(value) 和 $A$(Advantage)
- $Q(s,a)$ == $Return(s,a)$
- $V(s) = E_a(Q(s, a))$
- $A(s, a) = Q(s, a) - V(s)  
实际应用中，使用 A 更新 policy 会更稳定。

### 4. 区分 policy 和 Q/V/A
- 在状态 s 下，policy 选出用于与环境交互的行为 a。policy 选择行为 a 的依据是 Q(s,a) / V(s') ( 在状态 s 下执行 a 后转移到状态 s')。最优策略是 argmax(Q(s,a)) ，sample from distribution
- Q(s,a) / A(s,a) / V(s') 的更新依赖于policy

### 5. 区分MC, DP, n-steps TD, GAE
- DP 是已知 s,a->s'的状态转移概率，直接计算被估计值
- MC 和 TD 都是通过采样估计值
- MC 估计的样本全部来自采样，n-step TD 估计在第n步时使用估计值（有偏）
- GAE 是对 n-steps TD 估计 Advantage值 的优化，将不同n值的TD 估计以decay的方式糅合在一起

### 6. 区分 policy-based 和 value-based
- value-based：问题建模是求解Q/V/A，然后使用Q/V/A来确定 policy。
- policy-based：问题建模是直接求解策略 $\pi$

### 7. 区分离散和连续
- 理论上，在确定的 policy 下( eg, max )，可采样估计出所有的 Q/V， eg, Q-learning
- 实际上，当状态空间连续(eg, Atari)，或状态和行为空间均连续(eg, Mujoco)时，估计所有 Q/V成本过高，无法实现，因此引入DNN进行近似（DQN, DDPG）

### 8. 区分online和offline
- online：是线上训练，即便使用模型，边训练模型。
- offline：是线下训练，即使用训练好的模型。

### 9. on-policy和off-police
- on-policy指：计算Return时所采用的sample，均由policy采样所得。
- off-police指：计算Return时所采用的sample，并非由policy采样所得。
- 对比Q-learning(off-police)和SARSA(on-policy)可以更直观地看出二者的差异。左(q-learning), update Q时直接用的max Q(s’, a’)，右(SARSA)，使用采样的Q(s’, a’）

### 10. 区分 model-free 和 model-based
model 的意思是 environment。
- model-free：均将environment视为黑盒 不建模 $Psa(s')$
- model-based：需要显示计算出 $Psa(s')$